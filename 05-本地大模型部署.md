# 本地大模型部署

## 前置条件

* ollma工具https://ollama.com/
* wsl2-ubuntu22.04

## 具体步骤

```bash
# 下载ollma
curl -fsSL https://ollama.com/install.sh | sh
# 下载模型
ollama run llama3
ollama run qwen2
# 即可开始了

# 安装docker
# 清理历史包
for pkg in docker.io docker-doc docker-compose docker-compose-v2 podman-docker containerd runc; do sudo apt-get remove $pkg; done
# 
apt-get install ca-certificates curl
install -m 0755 -d /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
chmod a+r /etc/apt/keyrings/docker.asc
echo   "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
  $(. /etc/os-release && echo "$VERSION_CODENAME") stable" |   sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
apt update
apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
# 运行hello world测试安装
docker run hello-world

# 安装web UI
# 需要跟大模型联通，所以这里使用host网卡，测试，wsl2自带GPU，所以这里的--gpus不用也行
docker run -d \
 --network=host \
 -v open-webui:/app/backend/data \
 -e OLLAMA_BASE_URL=http://127.0.0.1:11434 \
 --name open-webui \
 --restart always \
 ghcr.io/open-webui/open-webui:main

# 其他open webui官方搭建命令
docker run -d \
 -p 8090:8080 \
 --add-host=host.docker.internal:host-gateway \
 -v open-webui:/app/backend/data \
 --name open-webui \
 --restart always \
 ghcr.io/open-webui/open-webui:main

docker run -d \
 -p 8090:8080 \
 --add-host=host.docker.internal:host-gateway \
 -v open-webui:/app/backend/data \
 -e OLLAMA_BASE_URL=http://127.0.0.1:11434 \
 --gpus all \
 --name open-webui \
 --restart always \
 ghcr.io/open-webui/open-webui:main

docker run -d \
 --gpus all \
 --network=host \
 -v open-webui:/app/backend/data \
 -e OLLAMA_BASE_URL=http://127.0.0.1:11434 \
 --name open-webui \
 --restart always \
 ghcr.io/open-webui/open-webui:main

```

```powershell
# 配置端口转发 wsl场景需要
netsh interface portproxy add v4tov4 listenport=8080 connectaddress=172.23.202.130
```

## 监控

```bash
# GPU使用率 %
nvidia-smi -q -d UTILIZATION | grep Gpu|awk -F":" '{print $2}'|awk -F" " '{print $1}'

# 内存使用率 %
nvidia-smi -q -d UTILIZATION | grep -m 1 Memory|awk -F":" '{print $2}'|awk -F" " '{print $1}'

# 功率监控 W
nvidia-smi -q -d POWER |grep "Power Draw"|head -n 1|awk -F":" '{print $2}'|awk -F" " '{print $1}'

# 温度监控 C
nvidia-smi -q -d TEMPERATURE|grep "GPU Current Temp"|awk -F":" '{print $2}'|awk -F" " '{print $1}'

# 内存监控
nvidia-smi -q -d MEMORY|grep -A 4 "FB Memory Usage"|grep Total|awk -F":" '{print $2}'|awk -F" " '{print $1}
nvidia-smi -q -d MEMORY|grep -A 4 "FB Memory Usage"|grep Reserved|awk -F":" '{print $2}'|awk -F" " '{print $1}
nvidia-smi -q -d MEMORY|grep -A 4 "FB Memory Usage"|grep Used|awk -F":" '{print $2}'|awk -F" " '{print $1}
nvidia-smi -q -d MEMORY|grep -A 4 "FB Memory Usage"|grep Free|awk -F":" '{print $2}'|awk -F" " '{print $1}

# nvidia-smi -q -d UTILIZATION
# nvidia-smi -q -d TEMPERATURE
# nvidia-smi -q -d CLOCK
# nvidia-smi -q -d COMPUTE
```

### zabbix脚本

```bash
#!/bin/bash

# Define the functions
function get_cpu_usage() {
  cpu_usage=`nvidia-smi -q -d UTILIZATION | awk '/Gpu/{print $(NF-1)}'`
  echo $cpu_usage;
}

function get_memory_usage() {
  memory_usage=`nvidia-smi -q -d UTILIZATION | awk '/Memory/'|awk 'NR==1 {print $(NF-1)}'`
  echo $memory_usage
}

function get_power() {
  powerw=`nvidia-smi -q -d POWER | awk '/Power Draw/'|awk 'NR==1 {print $(NF-1)}'`
  echo $powerw
}

function get_memory_info() {
  local param1=${1:-Total}
  memroy_info=`nvidia-smi -q -d MEMORY|grep -A 4 "FB Memory Usage"|awk "/$param1/"|awk 'NR==1 {print $(NF-1)}'`
  echo $memroy_info
}

function get_temp() {
  tempinfo=`nvidia-smi -q -d TEMPERATURE|awk '/GPU Current Temp/'|awk 'NR==1 {print $(NF-1)}'`
  echo $tempinfo
}

# Main script
case $1 in
  cpuusage) # Scenario A
    get_cpu_usage
    ;;
  memoryusage) # Scenario B
    get_memory_usage
    ;;
  power) # Scenario B
    get_power
    ;;
  memoryinfo) # Scenario B
    get_memory_info $2
    ;;
  temp) # Scenario B
    get_temp
    ;;
  \\?) # Unknown option
    echo "Error"
    ;;
esac
```

## zabbix-agent2配置

```bash
# /etc/zabbix# cat zabbix_agent2.conf |grep nv
UserParameter=nvcpuusage,/usr/bin/bash /etc/zabbix/nv_mon.sh cpuusage
UserParameter=nvmemusage,/usr/bin/bash /etc/zabbix/nv_mon.sh memoryusage
UserParameter=nvpower,/usr/bin/bash /etc/zabbix/nv_mon.sh power
UserParameter=nvtemp,/usr/bin/bash /etc/zabbix/nv_mon.sh temp
```

## powershell调用ollama

```powershell
## https://github.com/ollama/ollama/blob/main/docs/api.md

$ollamahost="http://172.23.202.130:11434"
$genContent="/api/generate"
$listModels="/api/tags"

### 列出本地模型列表

$url=$ollamahost+$listModels
$response = Invoke-WebRequest -Uri $url -Method Get

if ($response.StatusCode -eq 200) {
    Write-Host $response.Content
} else {
    Write-Error "Failed to get response from Ollama API."
}

$json=$response.content | convertfrom-json
$json.models.name

### 

$url=$ollamahost+$genContent

$bodyinfo=@"
{
  "model": "llama3",
  "prompt": "你好",
  "stream": false
}
"@

$response = Invoke-WebRequest -UseBasicParsing -ContentType 'application/json' -Uri $url -Method Post -Body $bodyinfo
$json=$response.content|ConvertFrom-json
$json.response

curl http://172.23.202.130:11434/api/generate -d '{
  "model": "llama3",
  "prompt": "Why is the sky blue?",
  "stream": false
}'
```